{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sys import platform\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "seed=42\n",
    "n_class=\"6class\"\n",
    "\n",
    "class_dist = {0:1000,1:1000,2:877,3:555,4:1000,5:1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id','start_time','mid_ts','label','audio','vggish']\n",
    "d=np.load(f'./resources/working_data/vocal_only_data_with_vggish.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "\n",
    "feature_df=df[['label','audio','band_name']]\n",
    "feature_df.groupby('label')['audio'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id','start_time','mid_ts','label','audio','vggish']\n",
    "d=np.load(f'./resources/working_data/vocal_only_data_with_vggish.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "\n",
    "feature_df=df[['label','audio','band_name','video_id']]\n",
    "mapping=[]\n",
    "for index,row in feature_df.iterrows():\n",
    "    if row['label'] == 'clean':\n",
    "        mapping.append(0)\n",
    "    if row['label'] == 'highfry':\n",
    "        mapping.append(1)\n",
    "    if row['label'] == 'layered':\n",
    "        mapping.append(1)\n",
    "    if row['label'] == 'lowfry':\n",
    "        mapping.append(1)\n",
    "    if row['label'] == 'midfry':\n",
    "        mapping.append(1)\n",
    "    if row['label'] == 'no_vocals':\n",
    "        mapping.append(2)\n",
    "\n",
    "feature_df.insert(3,'label_mapped',mapping)\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['audio','band_name','video_id']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label_mapped'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "video_ids=X_under[:,2]\n",
    "band_names = X_under[:,1]\n",
    "# X_under=X_under[:,0]#.reshape(-1,1).flatten()\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "# train_video_ids=video_ids[train]\n",
    "# test_video_ids=video_ids[test]\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "train_bands = X_train[:,1]\n",
    "test_bands = X_test[:,1]\n",
    "valid_bands = X_valid[:,1]\n",
    "\n",
    "train_songs = X_train[:,2]\n",
    "test_songs = X_test[:,2]\n",
    "valid_songs = X_valid[:,2]\n",
    "\n",
    "X_train = X_train[:,0]\n",
    "X_test = X_test[:,0]\n",
    "X_valid = X_valid[:,0]\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['y_train'] = y_train\n",
    "d['blah'] = 1\n",
    "print('TRAIN')\n",
    "print(d.groupby('y_train')['blah'].count())\n",
    "train = d['blah'].sum()\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['y_test'] = y_test\n",
    "d['blah'] = 1\n",
    "print('TEST')\n",
    "print(d.groupby('y_test')['blah'].count())\n",
    "test = d['blah'].sum()\n",
    "d=pd.DataFrame()\n",
    "d['y_valid'] = y_valid\n",
    "d['blah'] = 1\n",
    "print('VALID')\n",
    "print(d.groupby('y_valid')['blah'].count())\n",
    "valid = d['blah'].sum()\n",
    "\n",
    "print(f\"Train:Test:Validation - {train}:{test}:{valid}\")\n",
    "\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-rawaudio.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-rawaudio.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-rawaudio.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-rawaudio.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-rawaudio.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-rawaudio.npy', y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id','start_time','mid_ts','label','audio','vggish']\n",
    "d=np.load(f'./resources/working_data/vocal_only_data_with_vggish.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "\n",
    "feature_df=df[['label','audio','band_name','video_id']]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['audio','band_name','video_id']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "band_names = X_under[:,1]\n",
    "# X_under=X_under[:,0]#.reshape(-1,1).flatten()\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['y_train'] = y_train\n",
    "d['blah'] = 1\n",
    "print('TRAIN')\n",
    "print(d.groupby('y_train')['blah'].count())\n",
    "train = d['blah'].sum()\n",
    "\n",
    "d=pd.DataFrame()\n",
    "d['y_test'] = y_test\n",
    "d['blah'] = 1\n",
    "print('TEST')\n",
    "print(d.groupby('y_test')['blah'].count())\n",
    "test = d['blah'].sum()\n",
    "d=pd.DataFrame()\n",
    "d['y_valid'] = y_valid\n",
    "d['blah'] = 1\n",
    "print('VALID')\n",
    "print(d.groupby('y_valid')['blah'].count())\n",
    "valid = d['blah'].sum()\n",
    "\n",
    "print(f\"Train:Test:Validation - {train}:{test}:{valid}\")\n",
    "\n",
    "train_bands = X_train[:,1]\n",
    "test_bands = X_test[:,1]\n",
    "valid_bands = X_valid[:,1]\n",
    "\n",
    "train_songs = X_train[:,2]\n",
    "test_songs = X_test[:,2]\n",
    "valid_songs = X_valid[:,2]\n",
    "\n",
    "X_train = X_train[:,0]\n",
    "X_test = X_test[:,0]\n",
    "X_valid = X_valid[:,0]\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-rawaudio.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-rawaudio.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-rawaudio.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-rawaudio.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-rawaudio.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-rawaudio.npy', y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=np.load(f'./resources/working_data/vocal_only_data_with_vggish.npy',allow_pickle=True)\n",
    "\n",
    "feature_df=df[['label','vggish','band_name']]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['vggish','band_name']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "band_names = X_under[:,1]\n",
    "X_under=X_under[:,0]#.reshape(-1,1).flatten()\n",
    "X_under=np.concatenate(X_under).reshape(X_under.shape[0],128)\n",
    "\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "# for train_idx,test_idx in gss.split(X_under, y_under, band_names): \n",
    "#     print(train_idx,test_idx)\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-vggish.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-vggish.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-vggish.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-vggish.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-vggish.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-vggish.npy', y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id', 'start_time', 'mid_ts', 'label', 'average_zcr',\n",
    "       'zcr_stddev', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std',\n",
    "       'centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std','vggish']\n",
    "\n",
    "d=np.load(f'./resources/working_data/vocal_only_features.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "df\n",
    "\n",
    "feature_df=df[['label', 'band_name', 'average_zcr',\n",
    "       'zcr_stddev', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std',\n",
    "       'centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std']]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['average_zcr',\n",
    "       'zcr_stddev', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std',\n",
    "       'centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std','band_name']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "band_names = X_under[:,-1]\n",
    "X_under=X_under[:,:-1]#.reshape(-1,1).flatten()\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "# for train_idx,test_idx in gss.split(X_under, y_under, band_names): \n",
    "#     print(train_idx,test_idx)\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-features_unnormalized.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-features_unnormalized.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-features_unnormalized.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-features_unnormalized.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-features_unnormalized.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-features_unnormalized.npy', y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCCs only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id', 'start_time', 'mid_ts', 'label', 'average_zcr',\n",
    "       'zcr_stddev', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std',\n",
    "       'centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std','vggish']\n",
    "\n",
    "d=np.load(f'./resources/working_data/vocal_only_features.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "df\n",
    "\n",
    "feature_df=df[['label', 'band_name', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std']]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std','band_name']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "band_names = X_under[:,-1]\n",
    "X_under=X_under[:,:-1]#.reshape(-1,1).flatten()\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "# for train_idx,test_idx in gss.split(X_under, y_under, band_names): \n",
    "#     print(train_idx,test_idx)\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-mfcc_only_unnormalized.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-mfcc_only_unnormalized.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-mfcc_only_unnormalized.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-mfcc_only_unnormalized.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-mfcc_only_unnormalized.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-mfcc_only_unnormalized.npy', y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['video_id', 'start_time', 'mid_ts', 'label', 'average_zcr',\n",
    "       'zcr_stddev', 'mfcc1_mean', 'mfcc2_mean', 'mfcc3_mean',\n",
    "       'mfcc4_mean', 'mfcc5_mean', 'mfcc6_mean', 'mfcc7_mean', 'mfcc8_mean',\n",
    "       'mfcc9_mean', 'mfcc10_mean', 'mfcc11_mean', 'mfcc12_mean',\n",
    "       'mfcc13_mean', 'mfcc1_std', 'mfcc2_std', 'mfcc3_std', 'mfcc4_std',\n",
    "       'mfcc5_std', 'mfcc6_std', 'mfcc7_std', 'mfcc8_std', 'mfcc9_std',\n",
    "       'mfcc10_std', 'mfcc11_std', 'mfcc12_std', 'mfcc13_std',\n",
    "       'delta_mfcc1_mean', 'delta_mfcc2_mean', 'delta_mfcc3_mean',\n",
    "       'delta_mfcc4_mean', 'delta_mfcc5_mean', 'delta_mfcc6_mean',\n",
    "       'delta_mfcc7_mean', 'delta_mfcc8_mean', 'delta_mfcc9_mean',\n",
    "       'delta_mfcc10_mean', 'delta_mfcc11_mean', 'delta_mfcc12_mean',\n",
    "       'delta_mfcc13_mean', 'delta_mfcc1_std', 'delta_mfcc2_std',\n",
    "       'delta_mfcc3_std', 'delta_mfcc4_std', 'delta_mfcc5_std',\n",
    "       'delta_mfcc6_std', 'delta_mfcc7_std', 'delta_mfcc8_std',\n",
    "       'delta_mfcc9_std', 'delta_mfcc10_std', 'delta_mfcc11_std',\n",
    "       'delta_mfcc12_std', 'delta_mfcc13_std',\n",
    "       'centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std','vggish']\n",
    "\n",
    "d=np.load(f'./resources/working_data/vocal_only_features.npy',allow_pickle=True)\n",
    "df = pd.DataFrame(d,columns=cols)\n",
    "\n",
    "lut = pd.read_csv(f'../../dataset/lookup.csv')\n",
    "\n",
    "df=df.merge(lut[['video_id','band_name']],on='video_id')\n",
    "df\n",
    "\n",
    "feature_df=df[['label', 'band_name', 'average_zcr',\n",
    "       'zcr_stddev','centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std']]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=class_dist,random_state=seed)\n",
    "X = feature_df[['average_zcr',\n",
    "       'zcr_stddev','centroid_mean','centroid_std',\n",
    "       'contrast_mean','contrast_std',\n",
    "       'flatness_mean','flatness_std',\n",
    "       'rolloff_mean','rolloff_std','rms_mean','rms_std','band_name']].to_numpy()\n",
    "y=le.fit_transform(feature_df['label'].to_numpy())\n",
    "X_under, y_under = undersample.fit_resample(X, y)\n",
    "\n",
    "band_names = X_under[:,-1]\n",
    "X_under=X_under[:,:-1]#.reshape(-1,1).flatten()\n",
    "y_under=y_under\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=5, train_size=.7, random_state=seed)\n",
    "train,test = next(gss.split(X_under, y_under, band_names))\n",
    "# for train_idx,test_idx in gss.split(X_under, y_under, band_names): \n",
    "#     print(train_idx,test_idx)\n",
    "\n",
    "X_train = X_under[train]\n",
    "X_test1 = X_under[test]\n",
    "\n",
    "y_train = y_under[train]\n",
    "y_test1 = y_under[test]\n",
    "\n",
    "X_test,X_valid,y_test,y_valid = train_test_split(X_test1, y_test1,test_size=0.5,random_state=seed)\n",
    "\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "y_valid_hot = to_categorical(y_valid)\n",
    "\n",
    "X_train,y_train=shuffle(X_train,y_train_hot, random_state=seed)\n",
    "X_test,y_test=shuffle(X_test,y_test_hot, random_state=seed)\n",
    "X_valid,y_valid=shuffle(X_valid,y_valid_hot, random_state=seed)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_x_train-features_only_unnormalized.npy', X_train)\n",
    "np.save(f'./resources/working_data/{n_class}_x_test-features_only_unnormalized.npy', X_test)\n",
    "np.save(f'./resources/working_data/{n_class}_x_valid-features_only_unnormalized.npy', X_valid)\n",
    "\n",
    "np.save(f'./resources/working_data/{n_class}_y_train-features_only_unnormalized.npy', y_train)\n",
    "np.save(f'./resources/working_data/{n_class}_y_test-features_only_unnormalized.npy', y_test)\n",
    "np.save(f'./resources/working_data/{n_class}_y_valid-features_only_unnormalized.npy', y_valid)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
